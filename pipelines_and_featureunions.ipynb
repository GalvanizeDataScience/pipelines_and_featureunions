{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda/Objectives\n",
    "1. What are Pipelines and FeatureUnions?\n",
    "1. Why should I care?\n",
    "1. Basic example of how they work.\n",
    "1. Best practices for writing custom Transformers.\n",
    "1. Note some of the weaknesses and forthcoming features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Pipelines and FeatureUnions?\n",
    "\n",
    "* Method for chaining multiple estimators into a single one.\n",
    "* Estimators might include models, transformations etc.\n",
    "* `FeatureUnion` takes calls estimators which returns columns in parallel and `np.hstack`s the results together.\n",
    "* `Pipeline` Applies a sequence of transforms in series.\n",
    "* They can be used together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Best Part of `sklearn`\n",
    "\n",
    "* Sub-best parts of `sklearn`\n",
    " * Supervised learning\n",
    " * Unsupervised learning\n",
    " * Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are Pipelines and FeatureUnions so great?\n",
    "* Encourage good habits like:\n",
    " * separation of concerns\n",
    "     * cross-validation, development/computation\n",
    " * avoiding target-leakage by not accepting information about y in `transform`.\n",
    " * object orientedness\n",
    "* Promotes modeling choices to parameters\n",
    "* Readability\n",
    "    * Separates implementation details from general approach.\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are Pipelines and FeatureUnions so great?\n",
    "![CRISP-DM Process Diagram](images/440px-CRISP-DM_Process_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "CRISP-DM is a formalization of the way many Data Scientists work. Pipelines serve to expedite two parts of this workflow: the back-and-forth between data preparation and modeling and in turn, the overall cyclic process.\n",
    "\n",
    "Removing friction in this process means Data Scientists can explore more ideas, which is ultimately the process that leads to big gains in model performance. Note that data preparation involves feature engineering, which IMO is the single most important way to improve model performance in settings where the possibility of acquiring more data is real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do they work?\n",
    "\n",
    "* Initialize with a list of (name, estimator) tuples.\n",
    "* All but the last of these estimators must implement transform method.\n",
    "* From the docs: \"Calling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "#Pipelines are initialized with a list of (name, estimator) tuples.\n",
    "estimators = [('reduce_dim', PCA()), ('svm', SVC())]\n",
    "clf = Pipeline(estimators)\n",
    "clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classes_',\n",
       " 'decision_function',\n",
       " 'fit',\n",
       " 'fit_predict',\n",
       " 'fit_transform',\n",
       " 'get_params',\n",
       " 'inverse_transform',\n",
       " 'named_steps',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'steps',\n",
       " 'transform']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(clf) if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our pipeline object has chained two estimators together and given us a single consistent interface for both. It turns out that it's very powerful to be able to think of our complex, multi-step model as a single model. Especially when it comes to CV, which because we do not assume independence of our parameters requires us to search all parameters together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Writing custom transformers\n",
    "`sklearn` implements lots of good transformers, there are infinitely many more we may want to have so we'll often want to write our own.\n",
    "\n",
    "## The basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Recommended signature for a custom transformer.\n",
    "    \n",
    "    Inheriting from TransformerMixin gives you fit_transform\n",
    "    \n",
    "    Inheriting from BaseEstimator gives you grid-searchable params.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"If you need to parameterize your transformer,\n",
    "        set the args here.\n",
    "        \n",
    "        Inheriting from BaseEstimator introduces the constraint\n",
    "        that the args all be named keyword args, no positional \n",
    "        args or **kwargs.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        fit method.\n",
    "        \n",
    "        Set state here with whatever information\n",
    "        is needed to transform later.\n",
    "        \n",
    "        In some cases fit may do nothing. For example transforming \n",
    "        degrees Fahrenheit to Kelvin, requires no state.\n",
    "        \n",
    "        You can use y here, but won't have access to it in transform.\n",
    "        \"\"\"\n",
    "        #You have to return self, so we can chain!\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        transform method.\n",
    "        \n",
    "        Use state (if any) to transform some X data. This X\n",
    "        may be the same X passed to fit, but it may also be new data,\n",
    "        as in the case of a CV dataset. Both are treated the same.\n",
    "        \"\"\"\n",
    "        #Do transforms.\n",
    "        #transformed = foo(X)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice:\n",
    "\n",
    "Re-implement StandardScaler using the above stub.\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance:\n",
    "\n",
    "$$ \\frac{X - E(X)}{\\sigma(X)} $$\n",
    "\n",
    "## Practical hint:\n",
    "\n",
    "Call your transformer `MyScaler` and save it in `scaler.py` then you can run unittests in tests/test_scaler.py.\n",
    "\n",
    "Write your transformer from the notebook by:\n",
    "```python\n",
    "%%writefile scaler.py\n",
    "\n",
    "class MyScaler:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Then to run the tests from the notebook:\n",
    "\n",
    "```\n",
    "!python -m unittest tests.test_scaler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "#Try running some unittests to see if it's working correctly.\n",
    "!python -m unittest tests.test_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load scaler.py\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class MyScaler(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Scale to zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        fit method.\n",
    "        \n",
    "        Set state in your transformer with whatever information\n",
    "        is needed to transform later.\n",
    "        \"\"\"\n",
    "        #You have to return self, so we can chain!\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.scale = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        transform method.\n",
    "        \n",
    "        Use state (if any) to transform some X data. This X\n",
    "        may be the same X passed to fit, but it may also be new data,\n",
    "        as in the case of a CV dataset. Both are treated the same.\n",
    "        \"\"\"\n",
    "        #Do transforms.\n",
    "        Xt = X.copy()\n",
    "        Xt -= self.mean\n",
    "        Xt /= self.scale\n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Union\n",
    "\n",
    "Calls `fit` and `transform` in parallel and `np.hstack`s the output together.\n",
    "\n",
    "`transformer_weights` can scale the terms in the feature union. Useful for grid searching in regularized settings.\n",
    "\n",
    "`n_jobs` arg can be used to get parallel computation.\n",
    "\n",
    "For some complex transformers, alignment may be tricky! Pandas is good at this, but not helpful here because `np.hstack` is called, which ignores indexes.\n",
    "\n",
    "Writing a generalizable transformer often means you will expect the correct column to be selected from your X matrix, oftentimes this means writing a selector, which is too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.66284</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.504107</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.55249</td>\n",
       "      <td>0.55249</td>\n",
       "      <td>0.326310</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.326310</td>\n",
       "      <td>0.420183</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3         4        5        6        7   \\\n",
       "0  0.00000  0.00000  0.00000  0.00000  0.391484  0.66284  0.00000  0.00000   \n",
       "1  0.00000  0.00000  0.55249  0.55249  0.326310  0.00000  0.00000  0.00000   \n",
       "2  0.36043  0.36043  0.00000  0.00000  0.212876  0.00000  0.36043  0.36043   \n",
       "\n",
       "        8        9        10        11        12   13  \n",
       "0  0.00000  0.00000  0.00000  0.391484  0.504107  4.0  \n",
       "1  0.00000  0.00000  0.00000  0.326310  0.420183  5.0  \n",
       "2  0.36043  0.36043  0.36043  0.212876  0.000000  9.0  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"What is your name?\", \n",
    "          \"What is your favorite color?\",\n",
    "          \"What is the airspeed velocity of an unladen swallow?\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class WordCounter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        split = np.vectorize(lambda x: len(x.split()))\n",
    "        return split(X)[:,np.newaxis]\n",
    "\n",
    "fu = FeatureUnion([('tfidf', TfidfVectorizer()),\n",
    "                  ('counter', WordCounter())])\n",
    "\n",
    "#Pretty display of the output.\n",
    "pd.DataFrame(fu.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
