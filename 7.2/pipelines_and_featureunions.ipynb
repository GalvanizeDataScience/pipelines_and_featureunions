{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn The Best Parts\n",
    "## Isaac Lemon Laughlin\n",
    "Lead Instructor, Principal Data Scientist\n",
    "\n",
    "[Galvanize Inc.](www.galvanize.com)\n",
    "\n",
    "[@lemonlaug](www.twitter.com/lemonlaug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.1'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Agenda/Objectives\n",
    "1. What are Pipelines and FeatureUnions?\n",
    "1. Why should I care?\n",
    "1. Basic example of how they work.\n",
    "1. Best practices for writing custom Transformers.\n",
    "1. Get one hairy example under our belts.\n",
    "1. Note some of the weaknesses and forthcoming features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. What are Pipelines and FeatureUnions?\n",
    "\n",
    "* Method for chaining multiple estimators into a single one.\n",
    "* Estimators might include models, transformations etc.\n",
    "* `FeatureUnion` takes calls estimators which returns columns in parallel and `np.hstack`s the results together.\n",
    "* `Pipeline` Applies a sequence of transforms in series.\n",
    "* They can (and should) be used together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The Best Part of `sklearn`\n",
    "\n",
    "* Other parts of `sklearn`\n",
    " * Supervised learning\n",
    " * Unsupervised learning\n",
    " * Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2. Why are Pipelines and FeatureUnions so great?\n",
    "* Encourage good habits like:\n",
    " * separation of concerns\n",
    "     * cross-validation, development/computation\n",
    " * avoiding target-leakage by not accepting information about y in `transform`.\n",
    " * object orientedness\n",
    "* Promotes modeling choices to parameters\n",
    "    * Transformation to linearize a feature\n",
    "    * Handling missing values\n",
    "    * Constructing features\n",
    "* Readability\n",
    "    * Separates implementation details from general approach.\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2. Why are Pipelines and FeatureUnions so great?\n",
    "![CRISP-DM Process Diagram](images/440px-CRISP-DM_Process_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "CRISP-DM is a formalization of the way many Data Scientists work. Pipelines serve to expedite two parts of this workflow: the back-and-forth between data preparation and modeling and in turn, the overall cyclic process.\n",
    "\n",
    "Removing friction in this process means Data Scientists can explore more ideas, which is ultimately the process that leads to big gains in model performance. Note that data preparation involves feature engineering, which IMO is the single most important way to improve model performance in settings where the possibility of acquiring more data is real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. How do they work?\n",
    "\n",
    "* Initialize with a list of (name, estimator) tuples.\n",
    "* All but the last of these estimators must implement `transform` method.\n",
    "* From the docs: <blockquote>Calling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "#Pipelines are initialized with a list of (name, estimator) tuples.\n",
    "estimators = [('reduce_dim', PCA()), ('svm', SVC())]\n",
    "clf = Pipeline(estimators)\n",
    "clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classes_',\n",
       " 'decision_function',\n",
       " 'fit',\n",
       " 'fit_predict',\n",
       " 'fit_transform',\n",
       " 'get_params',\n",
       " 'inverse_transform',\n",
       " 'named_steps',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'steps',\n",
       " 'transform']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(clf) if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduce_dim__n_components': 12, 'svm__kernel': 'linear'}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "#Grid searching!\n",
    "params = {'reduce_dim__n_components':[1,5,10,12,15,20],\n",
    "         'svm__kernel':['linear', 'rbf']}\n",
    "gs = GridSearchCV(clf, param_grid=params)\n",
    "#Random data\n",
    "from sklearn.datasets import make_classification\n",
    "gs.fit(*make_classification())\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So our pipeline object has chained two estimators together and given us a single consistent interface for both. It turns out that it's very powerful to be able to think of our complex, multi-step model as a single model. Especially when it comes to CV, which because we do not assume independence of our parameters requires us to search all parameters together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Writing custom transformers\n",
    "`sklearn` implements lots of good transformers, there are infinitely many more we may want to have so we'll often want to write our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Recommended signature for a custom transformer.\n",
    "    \n",
    "    Inheriting from TransformerMixin gives you fit_transform\n",
    "    \n",
    "    Inheriting from BaseEstimator gives you grid-searchable params.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"If you need to parameterize your transformer,\n",
    "        set the args here.\n",
    "        \n",
    "        Inheriting from BaseEstimator introduces the constraint\n",
    "        that the args all be named keyword args, no positional \n",
    "        args or **kwargs.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    ...\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "...\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        fit method.\n",
    "        \n",
    "        Set state here with whatever information\n",
    "        is needed to transform later.\n",
    "        \n",
    "        In some cases fit may do nothing. For example transforming \n",
    "        degrees Fahrenheit to Kelvin, requires no state.\n",
    "        \n",
    "        You can use y here, but won't have access to it in transform.\n",
    "        \"\"\"\n",
    "        #You have to return self, so we can chain!\n",
    "        return self\n",
    "... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "...   \n",
    "    def transform(self, X):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        transform method.\n",
    "        \n",
    "        Transform some X data, optionally using state set in fit. This X\n",
    "        may be the same X passed to fit, but it may also be new data,\n",
    "        as in the case of a CV dataset. Both are treated the same.\n",
    "        \"\"\"\n",
    "        #Do transforms.\n",
    "        #transformed = foo(X)\n",
    "        return transformed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice:\n",
    "\n",
    "Re-implement StandardScaler using the above stub.\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance:\n",
    "\n",
    "$$ \\frac{X - E(X)}{\\sigma(X)} $$\n",
    "\n",
    "## Practical hint:\n",
    "\n",
    "Call your transformer `MyScaler` and save it in `scaler.py` then you can run unittests in tests/test_scaler.py.\n",
    "\n",
    "Write your transformer from the notebook by:\n",
    "```python\n",
    "%%writefile scaler.py\n",
    "\n",
    "class MyScaler:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Then to run the tests from the notebook:\n",
    "\n",
    "```\n",
    "!python -m unittest tests.test_scaler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 3 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "#Try running some unittests to see if it's working correctly.\n",
    "!python -m unittest tests.test_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load scaler.py\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class MyScaler(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Scale to zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        fit method.\n",
    "        \n",
    "        Set state in your transformer with whatever information\n",
    "        is needed to transform later.\n",
    "        \"\"\"\n",
    "        #You have to return self, so we can chain!\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.scale = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Recommended signature for custom transformer's\n",
    "        transform method.\n",
    "        \n",
    "        Use state (if any) to transform some X data. This X\n",
    "        may be the same X passed to fit, but it may also be new data,\n",
    "        as in the case of a CV dataset. Both are treated the same.\n",
    "        \"\"\"\n",
    "        #Do transforms.\n",
    "        Xt = X.copy()\n",
    "        Xt -= self.mean\n",
    "        Xt /= self.scale\n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Union\n",
    "\n",
    "Calls `fit` and `transform` in parallel and `np.hstack`s the output together.\n",
    "\n",
    "`transformer_weights` can scale the terms in the feature union. Useful for grid searching in regularized settings.\n",
    "\n",
    "`n_jobs` arg can be used to get parallel computation.\n",
    "\n",
    "For some complex transformers, alignment may be tricky! Pandas is good at this, but not helpful here because `np.hstack` is called, which ignores indexes.\n",
    "\n",
    "Writing a generalizable transformer often means you will expect the correct column to be selected from your X matrix, oftentimes this means writing a selector, which is too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class WordCounter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        split = np.vectorize(lambda x: len(x.split()))\n",
    "        return split(X)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.66284</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.504107</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.55249</td>\n",
       "      <td>0.55249</td>\n",
       "      <td>0.326310</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.326310</td>\n",
       "      <td>0.420183</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3         4        5        6        7   \\\n",
       "0  0.00000  0.00000  0.00000  0.00000  0.391484  0.66284  0.00000  0.00000   \n",
       "1  0.00000  0.00000  0.55249  0.55249  0.326310  0.00000  0.00000  0.00000   \n",
       "2  0.36043  0.36043  0.00000  0.00000  0.212876  0.00000  0.36043  0.36043   \n",
       "\n",
       "        8        9        10        11        12   13  \n",
       "0  0.00000  0.00000  0.00000  0.391484  0.504107  4.0  \n",
       "1  0.00000  0.00000  0.00000  0.326310  0.420183  5.0  \n",
       "2  0.36043  0.36043  0.36043  0.212876  0.000000  9.0  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"What is your name?\", \n",
    "          \"What is your favorite color?\",\n",
    "          \"What is the airspeed velocity of an unladen swallow?\"]\n",
    "\n",
    "fu = FeatureUnion([('tfidf', TfidfVectorizer()),\n",
    "                  ('counter', WordCounter())])\n",
    "\n",
    "#Pretty display of the output.\n",
    "pd.DataFrame(fu.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Practice\n",
    "\n",
    "Write a feature union that takes a single-vector as input and returns 3 columns corresponding to the square-root transformation, the identity transformation, and the square transformation.\n",
    "\n",
    "_Hint:_ `sklearn.preprocessing.FunctionTransformer` takes a function as an argument and converts it to a simple Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11021486]\n",
      " [ 0.64792271]\n",
      " [ 0.19943493]\n",
      " [ 0.8543808 ]\n",
      " [ 0.71626191]]\n",
      "[[ 0.33198624  0.11021486  0.01214732]\n",
      " [ 0.80493646  0.64792271  0.41980384]\n",
      " [ 0.44658139  0.19943493  0.03977429]\n",
      " [ 0.92432722  0.8543808   0.72996655]\n",
      " [ 0.84632258  0.71626191  0.51303113]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "X = np.random.random((100,1))\n",
    "\n",
    "fu = FeatureUnion([('sqrt',FunctionTransformer(np.sqrt)),\n",
    "                  ('identity',FunctionTransformer()),\n",
    "                  ('square', FunctionTransformer(lambda x: x**2))])\n",
    "print(X[:5])\n",
    "print(fu.fit_transform(X)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notes/Direction\n",
    "\n",
    "## Efficiency\n",
    "Some grid search steps may duplicate a lot of work by fitting/transforming the same data repeatedly. Caching may be forthcoming.\n",
    "\n",
    "## Inverse Transforms\n",
    "If implemented, can be used.\n",
    "\n",
    "## Post-processing/transformations of y.\n",
    "Not currently available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/Train.zip\n",
      "  inflating: Train.csv               \n"
     ]
    }
   ],
   "source": [
    "!unzip data/Train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Practice\n",
    "\n",
    "Putting it all together: creating a matrix of heterogeneous data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesID</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>datasource</th>\n",
       "      <th>auctioneerID</th>\n",
       "      <th>YearMade</th>\n",
       "      <th>MachineHoursCurrentMeter</th>\n",
       "      <th>UsageBand</th>\n",
       "      <th>saledate</th>\n",
       "      <th>...</th>\n",
       "      <th>Undercarriage_Pad_Width</th>\n",
       "      <th>Stick_Length</th>\n",
       "      <th>Thumb</th>\n",
       "      <th>Pattern_Changer</th>\n",
       "      <th>Grouser_Type</th>\n",
       "      <th>Backhoe_Mounting</th>\n",
       "      <th>Blade_Type</th>\n",
       "      <th>Travel_Controls</th>\n",
       "      <th>Differential_Type</th>\n",
       "      <th>Steering_Controls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139246</td>\n",
       "      <td>66000</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>11/16/2006 0:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Conventional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139248</td>\n",
       "      <td>57000</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>4640.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>3/26/2004 0:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Conventional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesID  SalePrice  MachineID  ModelID  datasource  auctioneerID  YearMade  \\\n",
       "0  1139246      66000     999089     3157         121           3.0      2004   \n",
       "1  1139248      57000     117657       77         121           3.0      1996   \n",
       "\n",
       "   MachineHoursCurrentMeter UsageBand         saledate        ...         \\\n",
       "0                      68.0       Low  11/16/2006 0:00        ...          \n",
       "1                    4640.0       Low   3/26/2004 0:00        ...          \n",
       "\n",
       "  Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type  \\\n",
       "0                     NaN          NaN   NaN             NaN          NaN   \n",
       "1                     NaN          NaN   NaN             NaN          NaN   \n",
       "\n",
       "  Backhoe_Mounting Blade_Type Travel_Controls Differential_Type  \\\n",
       "0              NaN        NaN             NaN          Standard   \n",
       "1              NaN        NaN             NaN          Standard   \n",
       "\n",
       "  Steering_Controls  \n",
       "0      Conventional  \n",
       "1      Conventional  \n",
       "\n",
       "[2 rows x 53 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data from: https://www.kaggle.com/c/bluebook-for-bulldozers\n",
    "df = pd.read_csv('data/Train.zip')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Practice\n",
    "\n",
    "Here are some suggested transformers to use in building your pipeline:\n",
    "\n",
    "| Column | Transformer | Notes |\n",
    "| ------ | ----------- | ----- |\n",
    "| `UsageBand` | `sklearn.preprocessing.OneHotEncoder` | |\n",
    "| `YearMade` | `sklearn.preprocessing.Imputer` | May want to also add a dummy column noting which rows are affected |\n",
    "| `fiProductClassDesc` | `sklearn.preprocessing.CountVectorizer` | You may ultimately want to reduce dimensionality of this using NMF or PCA. | \n",
    "| `State` | `sklearn.preprocessing.OneHotEncoder` | |\n",
    "| `YearMade`, `SaleDate` | Create a custom transformer to compute the age at sale. | |\n",
    "| `SalePrice` | Create a custom transformer that takes the K most recent sales within a ModelID | Beware alignment/target leakage. Use `GroupBy.transform(lambda x: x.ffill)`. This is delicate, so feel free to ask for a hint. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#This item selector, usage demonstrated at\n",
    "# http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "# Can be important for practically implementing FeatureUnions.\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to sklearn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
